<!DOCTYPE HTML>
<!--
	Phantom by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Deciphering Big Data - Unit by unit</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<div class="inner">

							<!-- Logo -->
								<a href="DecipheringBigData.html" class="logo">
									<span class="symbol"><img src="images/logo.svg" alt="" /></span><span class="title"></span>
								</a>

							<!-- Nav -->
								<nav>
									<ul>
										<li><a href="#menu">Menu</a></li>
									</ul>
								</nav>

						</div>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<h2>Menu</h2>
						<ul>
							<li><a href="DecipheringBigData.html">Deciphering Big Data - home</a></li>
							
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						<div class="inner">
							<h1>Deciphering Big Data - Unit by Unit</h1>
							<h2>Notes and reflections from each unit</h2>
							<hr />
							

<p><strong>Unit 1: Introduction to Big Data Technologies and Data Management</strong></p>
<p>This unit focused on what characterises big data, with definitions linked to the 4 Vs (volume, variety, voracity and velocity) provided. In essence data has grown so large and complex that specific technologies are required to manage it. New technologies are generating increasingly large and complex datasets in variety of data formats. There are many data formats, including: Structured (Databases), Semi-Structured (HTML, XML) Quasi Structural (Data Streams); Unstructured (JSON). Machine readable data is generally the easiest data to access, human readable the more challenging.</p>
<p>The value of the data held in many formats is when the value of data wrangling becomes apparent as a key and large part of the data science pipeline. In order to get any meaning from most data it must first be formatted into a useful format for analysis, bad data quality dealt with and outliers identified.</p>

							<p><a href="U1 Lecturecast.docx">U1 Lecturecast notes - Big Data, Data Types and Data Formats</a></p>
							<p><a href="U1 Reading.docx">Notes from unit reading </a></p>
							
<p>&nbsp;<strong>Unit 2: Introduction to Data Types and Formats</strong></p>
<p>This unit continued the focus on techniques with Python that can be useful for data wrangling and other processes. Covering Python basics before moving on to data import from different formats.</p>
<p>Sarkar, T. &amp; Roychowdhury content focused on concepts around stacks and queues and how these are useful for applications to process requests. It then moved on to useful data analysis functions in Python using NumPy, Pandas and Matlab &ndash; emphasising the usefulness of arrays, data frames and how these can end up as visualisations of data.&nbsp;</p>
<p>Kazil &amp; Jarmul ran through the basics of Python such as data formats (strings, integers etc); data containers (lists, variables, etc) and some useful concepts and tools. The chapter then moved on to machine readable format such as CSV and XML before more challenging Excel format, which are not meant to be read by machines but can be with some extra work.</p>
<p><em>Unfortunately, a corruption of the hard drive on my personal computer meat that my notes and extensive amounts of code written in Jupyter Notebook on this unit and subsequent units are lost. A lesson in the value of backups and cloud / offsite backup storage relevant to both database systems and personal devices.</em></p>

														
<p><strong><br /> Unit 3: Data Collection and Storage</strong></p>
<p>This unit considered sources of data across many sectors, then ways to store that data from flat files through relational databases and into NoSQL object-based databases. It considered cloud-based storage and advanced data management (HADOOP). Further data collection methods with web scarping were also discussed.</p>
<p>As part of this unit, a data collection exercise scraping from the web was completed using Python. Whilst a useful activity, a larger scale web scraping project would need to be undertaken to further embed web scraping with Python as a personal skill.</p>


							<p><a href="U3 Web scraping.ipynb">Webscraping activity code - Python</a></p>
							<p><a href="U3 Webscraping.docx">Notes from webscarping activity</a></p>
							
<p><strong><br /> Unit 4: Data Cleaning and Transformation</strong></p>
<p>This unit returned to data cleaning, wrangling and processing in order to get data into as good a format as possible for storage and analysis. This process is almost always undertaken after data capture. Key take aways were that data is rarely found in a perfect format, understanding the source and how processed it has been and then evaluating it are all key skills. The reading and coding exercises showed some practical examples of data cleaning and considerations when working to ensure data is fit for purpose. It highlights that even well-structured data can have issues and readability problems and needs cleaning. Problems like duplicates and nulls need to be considered as to how they will impact on analysis and how they should be dealt with.</p>
<p>Automating clean-up process, particularly if they are done repeatedly, can be huge time saver for analysts.</p>

							<p><a href="U4 Data Cleaning.ipynb">Webscraping activity code - Python</a></p>
							<p><a href="U4 Lecturecast and reading.docx">Notes from Lecturecast - Cleaning</a></p>
							<p><a href="U4 Reading.docx">Notes from unit reading</a></p>
							
<p><strong><br /> Unit 5: Data Cleaning and Automating Data Collections</strong></p>
<p>This week continued the practical exercises using Python to automate data cleanup. The reading emphasised that management of the data pipeline is hugely important in data analytics projects and data science. Being able to manage and automate those processes via Python or emerging technology like Data Ops can accelerate that process and make it more re-usable and resilient.&nbsp;</p>
<p>Case studies were also provided that offered examples of how to obtain data and validate the sources to allow questions to be answered. The examples showed the importance of questioning the reliability of data collection sources to understand if they are suitable for the analysis and making sure to find contacts and information about data sources to understand reliability and obtain context. &nbsp;</p>

							<p><a href="U5 Case Studies on Data Investigations.docx">Case Studies on Data Investigations</a></p>
							<p><a href="U5 Reading.docx">Notes from unit reading</a></p>
							
<p><strong><br /> Unit 6: Database Design and Normalisation</strong></p>
<p>The unit focused on database design best practice and particularly normalisation. The concepts on normalisation built on my knowledge from working in a professional environment. I have worked with and built data models that apply most of these concepts and can recognise where they have been applied and not applied. The idea of building the database to avoid the issues with poorly collected data is a strong concept to take from the module as it allows greater access to the data to perform analytics.</p>
<p>The reading included further coding exercises which showed a full data wrangling flow and very practical application for using pandas for preparing and making data fit for analysis. Simple process of checking data, removing redundancy, filling gaps, visualising and converting data types were all useful. The exercises also taught me some new skills of how to write back to a database file using Python.</p>

                                                        <p><a href="U6 Data Warngling.ipynb">Data Wrangling Activity - Python</a></p>
							<p><a href="U6 Lecturecast - Database Design Normalisation.docx">Lecturecast - Database Design (Normalisation)</a></p>
							<p><a href="Unit 6 Reading.docx">Notes from unit reading</a></p>

							
<p><strong><br /> Unit 7: Constructing Normalised Tables and Database Build</strong></p>
<p>This unit was an opportunity to implement practically some of the concepts explored already around database design and normalisation. Theoretically normalisation design and a chance to code in SQL were completed. This provided a formal structure to understand many concepts I had already encountered during my professional career; a well-structured database improves the quality and integrity of data and ultimately helps with data extraction and analysis. It can also have benefits for reducing data redundancy which assists in system performance and reduction of storage use.&nbsp;&nbsp; &nbsp;&nbsp;</p>

                                                        <p><a href="U7 DBM build.xlsx">Database Build Exercise </a></p>
						        <p><a href="U7 Normalisation Task.xlsx">Database Normalisation Task</a></p>	
							<p><a href="U7databasebuild.sql">SQL code for database build</a></p>
							<p><a href="U7 Reading.docx">Notes from unit reading</a></p>

							
<p><strong><br /> Unit 8: Compliance and Regulatory Framework for Managing Data</strong></p>
<p>This module focused on the laws and regulations that any organisation working with data must adhere. These apply to how data is stored, processed, secured and shared and should be a consideration when designing and implementing a system. Standards of system and security not only help organisations to process data lawfully to protect citizens rights but due to the value of data to organisations, keeping it safe and secure is vital. Some data is unique and could not be re-created if lost. Big data projects should be considering compliance from the outset to ensure it is designed into systems and process.</p>

					<p><a href="U8 Lecturecast Compliance.docx">Lecturecast - Compliance</a></p>
							<p><a href="U8 reading.docx">Notes from unit reading</a></p>

							
<p><strong><br /> Unit 9: Database Management Systems (DBMS) and Models</strong></p>
<p>Unit 9 covered in detail database management systems and their advantages (and disadvantages) as well as considerations and practical steps when planning and implementing a database. This technical knowledge of how to document a database and what it&rsquo;s constituent parts and outputs are required knowledge to successfully implement a system.</p>

<p><a href="U9 Lecturecast.docx">Lecturecast - Database Models and Systems</a></p>

							
<p><strong><br /> Unit 10: More on APIs (Application Programming Interfaces) for Data Parsing</strong></p>
<p>This unit explained what an API (application programming interface) is and why they are so important for the modern digitally connected world. API use is extensive and absolutely critical for the connected world we take for granted. Accessing data in its varied big data formats and putting it to use is one of the main functions of APIs across hugely varied sectors. Connecting different systems with different data and purposes in to one platform enhances the user experience and opportunities for organisations.&nbsp; Examples were given in the reading of how critical APIs are for the communication and updates people take for granted. APIs can move data between two separate software solutions can be implemented to result in a better overall solution.</p>

					<p><a href="U10 Case Study.docx">API Case Study</a></p>
							<p><a href="U10 Reading.docx">Notes from unit reading</a></p>
							
<p><strong><br /> Unit 11: DBMS Transaction and Recovery</strong></p>
<p>This module covered the technicalities of database transaction processing options and different approaches to backing up data. Transactions processing fully completing and backups are both important in ensuring data is not lost. Database management system design ensures that transactions are either fully completed or not complete at all, half processed data would lead to errors and corrupt data.</p>
<p>The backup procedure activity allowed different approaches to backups to be critically evaluated to consider the relative merits of various options. The best backup solution will always be driven by the business needs, how often is the data refreshed, how large is the data, what is the business risk of losing some of the data or not being able to recover quickly.</p>
<p>The reading and coding examples were both a refresher on SQL and a demonstration of the ability to utilise SQL in Python to create databases and querying them to provide a high level of flexibility to be able to wrangle, create, manage and query data.</p>

							   <p><a href="U1 RDMS and SQL.ipynb">SQL and RDMS code - Python</a></p>
	                                                       <p><a href="U11 Back up Procedure.docx">Backup procedures - critical analysis</a></p>
								<p><a href="U11 Database Transaction and Recovery.docx">Database Transaction and Recovery</a></p>
							<p><a href="U11 Reading.docx">Notes from unit reading</a></p>
							   		
<p><strong><br /> </strong><strong>Unit 12: Future of Big Data Analytics</strong></p>
<p>The final week emphasised that data management concepts and underlying technology is evolving rapidly and that increases emphasis on using data for decision making. The growth of more varied data types is also driving the growth in different database technologies, the value of this data becomes more of a focus so the need to store and utilise grows. The use of Machine learning to help solve complex data related problems is still a developing field. Aim to analyse big data with the aim of identifying new patterns and meaning to help with strategic decision making.</p>
<p>The module also covered big data best practice for organisations. Starting small with a defined problem that big data analysis could help with can prove the concept and value. It must be linked to a business process with an end point that has a high value that can be seen by stakeholders. Without organisational buy in and proof of worth, something technically brilliant will not just be a success even if it works elsewhere. &nbsp;</p>
<p>&nbsp;</p>

		<p><a href="U12 Lecturecast.docx">Lecturecast - The Future of Big Data Analytics</a></p>
							
<p><strong>Group Project</strong></p>
<p>These two documents were my personal contributions to the Development Team Project, they are not complete project reports.</p>

<h2><a href="Relect.html">Click here to read my relections on Deciphering Big Data </a></h2>
							

						</div>
					</div>

				<!-- Footer -->
					<footer id="footer">
					
							</section>
							<ul class="copyright">
						Design: <a href="http://html5up.net">HTML5 UP</a>
							</ul>
						</div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
