<!DOCTYPE HTML>
<!--
	Phantom by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Deciphering Big Data</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<div class="inner">

							<!-- Logo -->
								<a href="DecipheringBigData.html" class="logo">
									<span class="symbol"><img src="images/logo.svg" alt="" /></span><span class="title"></span>
								</a>

							<!-- Nav -->
								<nav>
									<ul>
										<li><a href="#menu">Menu</a></li>
									</ul>
								</nav>

						</div>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<h2>Menu</h2>
						<ul>
							<li><a href="index.html">Deciphering Big Data</a></li>
							<li><a href="Relect.html">Reflections on DBD</a></li>
							<li><a href="Units_DBD.html">Unit by Unit</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						<div class="inner">
							<h1>Reflections on DBD</h1>

							<hr />
<p>Big data is a term widely used but its specific definition and value was made clearer to me during this module. Ohlhorst (2012) characterises big data as being too large and complex for traditional technologies to comfortably handle. Rapid advances in technology have seen data grow in the size (volume); in speed of collection and need to make accessible as soon as possible (velocity); and in complexity (variety and voracity). These 4 Vs are key when considering the characteristics of data and therefore play an influencing factor when planning and implementing any big data project. The nature of the data will play a considerable factor in how a project will collect, clean, store and analyse it.</p>
<p>Data Wrangling is a key part of nearly all big data projects, this involves preparing data for analysis by cleaning formatting the raw data so it is free anomalies, fields are standardized and outliers identified (Kazil &amp; Jarmul, 2019). As a data professional I am aware that data is rarely ready for analysis from source, and that evaluation of the data followed by cleaning is a big part of any analysis project. The course re-emphasised how critical this part of the data pipeline is to a final quality output, with the value of a programming language like Python to provide flexible data wangling processes that are reproducible and automatable. Organisations like my own, are typically reliant on expensive enterprise solutions which do not have the same flexibility or power as Python as a data wrangling tool (Sarkar &amp; Roychowdhury, 2019). In a world where the pace of change for data is so rapid, being able to script your own data wrangling pipeline as required provides resilience to respond to that change. Communicating the value of Python to further enable Data Science implementation in my organisation is a goal I have set myself as a result of this module.</p>
<p>The module provided an opportunity to work through step-by-step Python exercise illustrating how to collect, clean and export data. As someone learning to use Python, this helped further embed some of the basic concepts of the language. This learning included how to set up a Python environment for analysis; concepts around handling different data formats and types; how to import data from different sources including the web; and further appreciation for how Python can enable data science. These activities involved re-typing code and were not always as engaging a learning approach for me as when there is a problem to overcome that requires pro-active learning and problem solving to achieve an end goal. This is a reflection for me on my learning style that I should find a way to put into practice the skills and techniques I read as core material with a realistic problem.</p>
<p>The team project gave me a chance to put into practice the module content by working through a typical database implementation process. Project planning was used to understand the objective of the system in terms of both business need and technical requirements. There was also a need to evaluate the format and nature of the data available and what methods could be used to collect that data and clean it so it was fit for further use. Understanding the data informed a decision about which storage option to use based on the strengths and limitations of different technology. This discovery work identified mainly structured data suited to a RDBMS. The detailed planning that took place particularly in identifying data sources and aligning them with business needs meant there was only minor change between planning and implementation, this demonstrates the value of research and understanding of the sources and business requirements when developing a system. There were some changes to data collection methods with it being identified during implementation that APIs could offer an alternative and automated method to parse data and integrate it with the platform. Practical considerations to cover legal considerations particularly the security of data was not applied until the implementation phase.</p>
<p>I suggested the initial project concept which was refined through collaboration and I took the lead for setting out the initial structure of each report to meet the brief and cover module topics, both were big contributions to the clarity the group had when working through the project. My direct contribution to the report focused on the identification of data sources to fulfil the business need and evaluation of the type and format of this data to enable the selection of the most suitable database solution to handle the data and meet requirements. This included detailed logical mapping of the data to distinct relations and entities including data types; identifying the links between relations; and ensuring normal form was applied. This detailed design of process allowed a case to be put forward that a SQL Relational Database Management was most suited to the available structured data which formed part of a logical customer contact process. &nbsp;It also suited the scale of data collected and rate of arrival into the platform. &nbsp;Therefore, meeting the aim to provide a platform that would collect data with more integrity and enable more business insight from analysis of the data. The implementation decision links back to the initial consideration of the 4Vs of big data set out at the outset of the module. The complexity, size, reliability, and range of data determines the approaches need to collect, clean, manage, secure and analyse the data. &nbsp;</p>
<p>The group worked well together on the project, topic areas were divided across the members and all completed their sections on time demonstrated knowledge of the topics. Open communication resulting in a division of responsibility which worked well for a peer dynamic in which everyone has an equal share of responsibility. Feedback to colleagues was constructive but in comparison to a workplace there no final quality assurance check from a management which might have yielded further improvements in the end product, perhaps a weakness of approach that could partially have been bridged by including a more structured peer review process. &nbsp;</p>
<p>The module has helped me to understand what big data is and appreciate the processes, technologies and considerations that are required to gain insight and make decisions from the data. As someone already in a role to influence the approach my organisation takes to data it has been invaluable for my continued professional development and in understanding what more I need to learn to further understand the rapidly changing world of big data.</p>
							<p><b>References:</b></p>
<p>Ohlhorst, F.J. (2012) Big Data Analytics: Turning Big Data into Big Money. Wiley Professional Development.</p>
<p>Kazil, J &amp; Jarmul, K. (2016) Data Wrangling with Python. O'Reilly. Media Inc.</p>
<p>Sarkar, T. &amp; Roychowdhury, S. (2019) Data Wrangling with Python. 1st ed. Packt.</p>

<h2><a href="Units_DBD.html">Click here to see a unit by unit run through of topics covered and exercises completed</a></h2>
							

						</div>
					</div>

				<!-- Footer -->
					<footer id="footer">
					
							</section>
							<ul class="copyright">
						Design: <a href="http://html5up.net">HTML5 UP</a>
							</ul>
						</div>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
